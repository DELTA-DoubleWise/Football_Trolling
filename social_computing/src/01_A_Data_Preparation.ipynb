{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from praw.models import MoreComments\n",
    "import datetime as dt\n",
    "from datetime import datetime,timedelta\n",
    "from psaw import PushshiftAPI\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import dill\n",
    "import tqdm.notebook as tq\n",
    "from utils import crawler_utils\n",
    "from utils.crawler_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()\n",
    "start_time, end_time = time_generation_year(0)\n",
    "end_time_0 = int(dt.datetime(2021,12,31).timestamp())\n",
    "start_time_0 = int(dt.datetime(2016,1,1).timestamp())\n",
    "#start_time, end_time = time_initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1609430400, 1612108800, 1614528000, 1617206400, 1619798400, 1622476800, 1625068800, 1627747200, 1630425600, 1633017600, 1635696000, 1638288000]\n"
     ]
    }
   ],
   "source": [
    "print(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_one_subreddit_post(subreddit_name,club_abbr):\n",
    "    temp_list = []\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        temp_list.append(np.array(list(api.search_submissions(after=start_time[i], before=end_time[i],\n",
    "                                                              subreddit=subreddit_name,\n",
    "                                                              filter=['url', 'author', 'title', 'subreddit', 'id',\n",
    "                                                                      'num_comments', 'score', 'ups', 'upvote_ratio','selftext'])\n",
    "                                       )))\n",
    "    file = open(f'../data/{club_abbr}.pickle', 'wb')\n",
    "    dill.dump(temp_list, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_post(club_abbr_list,club_reddit_list):\n",
    "    for club_index in tq.tqdm(range(len(club_abbr_list))):\n",
    "        collect_one_subreddit_post(club_reddit_list[club_index],club_abbr_list[club_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.metadata_.get('shards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for club_index in tq.tqdm(range(len(club_abbr))):\n",
    "    temp_list = []\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        temp_list.append(np.array(list(api.search_submissions(after=start_time[i], before=end_time[i],\n",
    "                                                              subreddit=club_reddit_abbr[club_index],\n",
    "                                                              filter=['url', 'author', 'title', 'subreddit', 'id',\n",
    "                                                                      'num_comments', 'score', 'ups', 'upvote_ratio','selftext'])\n",
    "                                       )))\n",
    "    file = open(f'../data/{club_abbr[club_index]}.pickle', 'wb')\n",
    "    dill.dump(temp_list, file)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Collection in Premier (other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_post(premier_other_abbr,premier_other_reddit_abbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_post(club_abbr_list):\n",
    "    for club_name in tq.tqdm(club_abbr_list):\n",
    "        filename = f'../data/{club_name}.pickle'\n",
    "        df = transform_df(filename)\n",
    "        df = parse_dates(df)\n",
    "        df.to_csv(f'../data/{club_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'../data/NEW.pickle'\n",
    "df = transform_df(filename)\n",
    "df = parse_dates(df)\n",
    "df.to_csv(f'../data/NEW.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_post(club_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_post(premier_other_abbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use praw (not recommended, stuck from time to time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['comments','post_id','subreddit','time','title']\n",
    "reddit = praw.Reddit(client_id='bCnE1U61Wqixgs2wy28POg', client_secret='vEY7k3_j7o3PZZvP-tEt6DnhWr1x5A',\n",
    "                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36')\n",
    "all_comments = pd.DataFrame(columns = columns)\n",
    "for club_name in tq.tqdm(club_abbr):\n",
    "    filename = f'../data/{club_name}.csv'    \n",
    "    all_comments.append(extract_comments(reddit,filename,columns),ignore_index=True)\n",
    "all_comments.to_csv('../data/all_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['comments','post_id','subreddit','time','title']\n",
    "reddit = praw.Reddit(client_id='bCnE1U61Wqixgs2wy28POg', client_secret='vEY7k3_j7o3PZZvP-tEt6DnhWr1x5A',\n",
    "                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36')\n",
    "all_comments = pd.DataFrame(columns = columns)\n",
    "for club_name in tq.tqdm(club_abbr):\n",
    "    filename = f'../data/{club_name}.csv'\n",
    "    comments = extract_comments_by_praw(reddit,filename,columns)\n",
    "    comments.to_csv(f'../data/comments/temp/{club_name}.csv',index=False)\n",
    "    #all_comments.append(extract_comments(reddit,filename,columns),ignore_index=True)\n",
    "#all_comments.to_csv('../data/all_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use psaw (extract comments one by one, too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['comments','post_id','subreddit','time','title']\n",
    "for club_name in tq.tqdm(club_abbr):\n",
    "    filename = f'../data/{club_name}.csv'    \n",
    "    comments = extract_comments_by_psaw(filename,columns)\n",
    "    comments.to_csv(f'../data/comments/{club_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use reddit api (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract comments of posts with over 50 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comments = pd.DataFrame()\n",
    "columns = ['text','id','post_id','subreddit','time','author','ups']\n",
    "for club_name in tq.tqdm(club_abbr):\n",
    "    filename = f'../data/{club_name}.csv'    \n",
    "    comments = extract_comments_by_api(filename,columns)\n",
    "    comments.to_csv(f'../data/comments/{club_name}_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract gameday comments (48hrs from the kick-off time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.DataFrame()\n",
    "columns = ['text','id','post_id','subreddit','time','author','ups']\n",
    "for club_name in tq.tqdm(premier_abbr):\n",
    "    filename = f'../data/{club_name}.csv'\n",
    "    matchfilename = f'../data/match/{club_name}_match.csv'\n",
    "    comments = extract_gameday_comments(filename,matchfilename,columns)\n",
    "    comments.to_csv(f'../data/comments/gameday/{club_name}_gameday_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tq.tqdm(range(19,len(premier_abbr))):\n",
    "    matchfilename = f'../data/match/{premier_abbr[i]}_match.csv'\n",
    "    subreddit = premier_reddit_abbr[i]\n",
    "    club_name = premier_abbr[i]\n",
    "    comments = get_gameday_comments(subreddit,matchfilename)\n",
    "    comments.to_csv(f'../data/comments/gameday/{club_name}_gameday_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Comments by player Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in target_player:\n",
    "    df = pd.DataFrame()\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(q = name, before=end_time[i], after=start_time[i], subreddit='ThreeLions',\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/player/{name}_national_team.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in target_player:\n",
    "    df = pd.DataFrame()\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(q = name, before=end_time[i], after=start_time[i], subreddit='PremierLeague',\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/player/{name}_Premier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in target_player:\n",
    "    df = pd.DataFrame()\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(q = name, before=end_time[i], after=start_time[i], subreddit='soccer',\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/player/{name}_soccer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in target_player:\n",
    "    df = pd.DataFrame()\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(q = name, before=end_time[i], after=start_time[i], subreddit='football',\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/player/{name}_football.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for player in range(len(target_player_1)):\n",
    "    df = pd.DataFrame()\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(q = target_player_1[player], before=end_time[i], after=start_time[i], subreddit=player_club[player],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/player/{target_player_1[player]}_{player_club[player]}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Comments by Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../data/author/euro_Kane_soccer_10.csv')\n",
    "author_list = authors['author']\n",
    "df = pd.read_csv('../data/comments/player/author/euro_Kane_soccer_author_comments.csv')\n",
    "#df = pd.DataFrame()\n",
    "for name in tq.tqdm(author_list):\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv('../data/comments/player/author/euro_Kane_soccer_author_comments.csv',index=False)\n",
    "df.to_csv('../data/comments/player/author/euro_Kane_soccer_author_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../data/author/Kane_Premier_author.csv')\n",
    "author_list = authors['author']\n",
    "#df = pd.read_csv('../data/comments/player/author/Kane_Premier_author_comments.csv')\n",
    "df = pd.DataFrame()\n",
    "for name in tq.tqdm(author_list):\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv('../data/comments/player/author/Kane_Premier_author_comments.csv',index=False)\n",
    "df.to_csv('../data/comments/player/author/Kane_Premier_author_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../data/author/Kane_TOT_author.csv')\n",
    "author_list = authors['author']\n",
    "df = pd.read_csv('../data/comments/player/author/Kane_TOT_author_comments.csv')\n",
    "#df = pd.DataFrame()\n",
    "for name in tq.tqdm(author_list):\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv('../data/comments/player/author/Kane_TOT_author_comments_1.csv',index=False)\n",
    "df.to_csv('../data/comments/player/author/Kane_TOT_author_comments_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../data/author/LIV_neg_author.csv')\n",
    "author_list = authors['author']\n",
    "#df = pd.read_csv('../data/comments/player/author/Kane_TOT_author_comments.csv')\n",
    "df = pd.DataFrame()\n",
    "for name in tq.tqdm(author_list):\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv('../data/comments/author_bg/LIV_neg_author_bg.csv',index=False)\n",
    "df.to_csv('../data/comments/author_bg/LIV_neg_author_bg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('../data/author/LIV_pos_author.csv')\n",
    "author_list = authors['author']\n",
    "#df = pd.read_csv('../data/comments/player/author/Kane_TOT_author_comments.csv')\n",
    "df = pd.DataFrame()\n",
    "for name in tq.tqdm(author_list):\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv('../data/comments/author_bg/LIV_pos_author_bg.csv',index=False)\n",
    "df.to_csv('../data/comments/author_bg/LIV_pos_author_bg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(f'../data/author/MNU_pos_author.csv')\n",
    "author_list = authors['author'][10:]\n",
    "df = pd.read_csv(f'../data/comments/author_bg/MNU_pos_author_bg.csv')\n",
    "for name in tq.tqdm(author_list):\n",
    "    for i in tq.tqdm(range(len(start_time))):\n",
    "        gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/author_bg/MNU_pos_author_bg.csv',index=False)\n",
    "df.to_csv(f'../data/comments/author_bg/MNU_pos_author_bg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_list = ['CHE','MNC','TOT']\n",
    "for team in team_list:\n",
    "    authors = pd.read_csv(f'../data/author/{team}_neg_author.csv')\n",
    "    author_list = authors['author']\n",
    "    #df = pd.read_csv(f'../data/comments/author_bg/{team}_neg_author_bg.csv')\n",
    "    df = pd.DataFrame()\n",
    "    for name in tq.tqdm(author_list):\n",
    "        for i in tq.tqdm(range(len(start_time))):\n",
    "            gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                      filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "            results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                       'author': row.author, 'body': row.body} for row in gen)\n",
    "            df = pd.concat([df, results_df])\n",
    "        df.to_csv(f'../data/comments/author_bg/{team}_neg_author_bg.csv',index=False)\n",
    "    df.to_csv(f'../data/comments/author_bg/{team}_neg_author_bg.csv',index=False)\n",
    "    \n",
    "    authors = pd.read_csv(f'../data/author/{team}_pos_author.csv')\n",
    "    author_list = authors['author']\n",
    "    #df = pd.read_csv(f'../data/comments/author_bg/{team}_pos_author_bg.csv')\n",
    "    df = pd.DataFrame()\n",
    "    for name in tq.tqdm(author_list):\n",
    "        for i in tq.tqdm(range(len(start_time))):\n",
    "            gen = api.search_comments(author = name, before=end_time[i], after=start_time[i],\n",
    "                                      filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "            results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                       'author': row.author, 'body': row.body} for row in gen)\n",
    "            df = pd.concat([df, results_df])\n",
    "        df.to_csv(f'../data/comments/author_bg/{team}_pos_author_bg.csv',index=False)\n",
    "    df.to_csv(f'../data/comments/author_bg/{team}_pos_author_bg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv(f'../data/author/Sterling_negative_author.csv')\n",
    "author_list = authors['author']\n",
    "#df = pd.read_csv(f'../data/comments/author_bg/player_group1_comments.csv')\n",
    "df = pd.DataFrame()\n",
    "for name in tq.tqdm(author_list):\n",
    "    for player in tq.tqdm(player_group1):\n",
    "        gen = api.search_comments(author = name, q=player,\n",
    "                                  filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "        results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                                   'author': row.author, 'body': row.body, 'player':player} for row in gen)\n",
    "        df = pd.concat([df, results_df])\n",
    "    df.to_csv(f'../data/comments/author_bg/player_group1_comments.csv',index=False)\n",
    "df.to_csv(f'../data/comments/author_bg/player_group1_comments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feegee2000/opt/anaconda3/lib/python3.7/site-packages/psaw/PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    }
   ],
   "source": [
    "gen = api.search_comments(author = 'aussie_spastic',\n",
    "                          filter=['author', 'body', 'subreddit', 'id', 'created_utc'])\n",
    "results_df = pd.DataFrame({'subreddit': row.subreddit, 'id': row.id, 'time': datetime.fromtimestamp(int(row.created_utc)),\n",
    "                           'author': row.author, 'body': row.body,} for row in gen)\n",
    "results_df.to_csv(f'../data/vis/author_bg/author2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_posts(club_abbr_list,output_dir):\n",
    "    all_posts = pd.DataFrame()\n",
    "    for club_name in club_abbr_list:\n",
    "        filename = f'../data/{club_name}.csv'\n",
    "        temp_data = pd.read_csv(filename)\n",
    "        all_posts = all_posts.append(temp_data)\n",
    "    all_posts.to_csv(f'../data/{output_dir}.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_posts(club_abbr,'all_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_posts(premier_abbr,'premier_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
