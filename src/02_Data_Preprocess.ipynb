{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from gensim import corpora, matutils, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sys\n",
    "import os\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all_posts.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, text_field,manager,stadium):\n",
    "    '''\n",
    "    Clean all the text data within a certain text column of the dataFrame.\n",
    "    '''\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"&[a-z]{2,4};\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(\"\\\\n\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"#f\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[\\’\\'\\`\\\":]\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\" +\", \" \")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    for word in manager:\n",
    "        df[text_field] = df[text_field].replace((\" \"+word+\" \"), \" managers \",regex=True)\n",
    "    for word in stadium:\n",
    "        df[text_field] = df[text_field].replace((\" \"+word+\" \"), \" stadium \",regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(\" nan\", \"\",regex=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df.self_text = df.self_text.astype(str)\n",
    "    df['text'] = df.title + ' ' + df.self_text\n",
    "    manager_names = pickle.load(open('../data/pretrained/manager_names.pkl','rb'))\n",
    "    stadium_names = pickle.load(open('../data/pretrained/stadium_names.pkl','rb'))\n",
    "    clean_text(df, 'text',manager_names,stadium_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = []\n",
    "for text in tq.tqdm(df.text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    lemma.append(lemmatized_output)\n",
    "df.text = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_nicknames = ['leo','cr7','chicharito','hulk','dave','rhino','rino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_names = pickle.load(open('../data/pretrained/player_names.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names = ['arsenal','gunners','milan','ac','acmilan','rossoneri','atlético','atletico','madrid','barcelona','barca',\n",
    "             'borussia','dortmund','borussiadortmund','schwarzgelben','chelsea','munich','bayern','münchen','inter',\n",
    "             'intermilan','nerazzurri','juve','juventus','liverpool','reds','manchester','mancity','city','mcfc','psg','paris',\n",
    "             'saint-germain','saint','germain','real','united','manutd','utd','man','mufc','devels','red','blue','roma',\n",
    "             'tottenham','spurs','spur','hotspur','hotspurs','aston', 'villa','brentford','beighton','burnley','cystal',\n",
    "             'palace','everton','leeds','leicester','newcastle','norwich','southampton','swansea','watford','ham','wolverhampton','wanderers',\n",
    "             'alavés','athletic','celta','vigo','elche','espanyol','getafe','granada','levante','mallorca','osasuna','vallecano',\n",
    "             'betis','sociedad','sevilla','valencia','villarreal','atlanta','bologna','cagliari','empoli','fiorentina','genoa',\n",
    "             'verona','internazionale','lazio','napoli','salernitana','sampdoria','sassuolo','spezia','torino','udinese','venezia',\n",
    "             'arminia','bielefeld','leverkusen','bochum','mönchengladbach','eintracht','frankfurt','freiburg','hertha','hoffenhiem',\n",
    "             'köln','leipzig','mainz','stuttgart','union','berlin','wolfsburg','lille','lyon','marseille','monaco','nantes','nice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_words = ['mon','tue','wed','thu','fri','sat','sun','jan','feb','mar','apr','may','jun','jul','aug','sep',\n",
    "              'oct','nov','dec','monday','tuesday','wednesday','thursday','friday','saturday','sunday','january',\n",
    "              'feburary','march','april','may','june','july','august','september','october','november','december',\n",
    "              '2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023',\n",
    "              '2024','2025','2026','2027']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_generation():\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend([x.replace(\"\\'\", \"\") for x in stop])\n",
    "    stop.extend(['nbsp', 'also', 'really', 'ive', 'even', 'jon', 'lot', 'could', 'many','x200b','would','one','remove','removed','delete','deleted',\n",
    "             'juventusnews24'])\n",
    "    stop.extend(player_names)\n",
    "    stop.extend(time_words)\n",
    "    stop.extend(player_nicknames)\n",
    "    stop.extend(team_names)\n",
    "    stop = list(set(stop))\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_generation():\n",
    "    stop = stopwords_generation()\n",
    "    cv = CountVectorizer(token_pattern='\\\\w{3,}', max_df=.30, min_df=.0001,\n",
    "                     stop_words=stop, ngram_range=(1,1),lowercase=False,dtype='uint8')\n",
    "\n",
    "    tfidf = TfidfVectorizer(token_pattern='\\\\w{3,}', max_df=.30, min_df=.0001, \n",
    "                        stop_words=stop, ngram_range=(1,1), lowercase=False,\n",
    "                        sublinear_tf=True, smooth_idf=False, dtype='float32')\n",
    "    cv_vecs = cv.fit_transform(df.text).transpose()\n",
    "    tf_vecs = tfidf.fit_transform(df.text).transpose()\n",
    "    print(\"Sparse Shape:\", cv_vecs.shape) \n",
    "    print('CV:', sys.getsizeof(cv_vecs))\n",
    "    print('Tf-Idf:', sys.getsizeof(tf_vecs))\n",
    "    pickle.dump(cv_vecs, open('../data/pretrained/cv_vecs_v4.pkl', 'wb'))\n",
    "    pickle.dump(tf_vecs, open('../data/pretrained/tfidf_vecs_v4.pkl', 'wb'))\n",
    "    return cv,tfidf,cv_vecs,tf_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv,tfidf,cv_vecs, tf_vecs = vector_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_info(cv_vecs,tf_vecs):\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(tf_vecs.transpose().todense(), columns=[tfidf.get_feature_names()]).astype('float32')\n",
    "    cv_df = pd.DataFrame(cv_vecs.transpose().todense(), columns=[cv.get_feature_names()]).astype('uint8')\n",
    "    print(cv_df.info())\n",
    "    print(tfidf_df.info())\n",
    "    cv_description = cv_df.describe().T\n",
    "    tfidf_description = tfidf_df.describe().T\n",
    "    print(tfidf_df.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trunc_SVD(vectorized, n_components=500, iterations=1, normalize=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs LSA/LSI on a sparse document term matrix, returns a fitted, transformed, (normalized) LSA object\n",
    "    \"\"\"\n",
    "    # Already own the vectorized data for LSA, just transpose it back to normal\n",
    "    vecs_lsa = vectorized.T\n",
    "\n",
    "    # Initialize SVD object as LSA\n",
    "    lsa = TruncatedSVD(n_components=n_components, n_iter=iterations, algorithm='randomized', random_state=random_state)\n",
    "    dtm_lsa = lsa.fit(vecs_lsa)\n",
    "    print(\"Explained Variance - LSA {}:\".format(n_components), dtm_lsa.explained_variance_ratio_.sum())\n",
    "    if normalize:\n",
    "        dtm_lsa_t = lsa.fit_transform(vecs_lsa)\n",
    "        dtm_lsa_t = Normalizer(copy=False).fit_transform(dtm_lsa_t)\n",
    "        return dtm_lsa, dtm_lsa_t\n",
    "    return dtm_lsa\n",
    "\n",
    "\n",
    "def plot_SVD(lsa, title, level=None):\n",
    "    \"\"\"\n",
    "    Plots the singular values of an LSA object\n",
    "    \"\"\"\n",
    "    plt.figure(num=1, figsize=(15,10))\n",
    "    plt.suptitle(title, fontsize=22, x=.55, y=.45, horizontalalignment='left')\n",
    "    plt.subplot(221)\n",
    "    plt.title('Explained Variance by each Singular Value')\n",
    "    plt.plot(lsa.explained_variance_[:level])\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.title('Explained Variance Ratio by each Singular Value')\n",
    "    plt.plot(lsa.explained_variance_ratio_[:level])\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.title(\"Singular Values ('Components')\")\n",
    "    plt.plot(lsa.singular_values_[:level])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dtm_lsa = Trunc_SVD(cv_vecs,iterations=5)\n",
    "plot_SVD(cv_dtm_lsa, title='Count Vectorizer', level=25)\n",
    "\n",
    "tf_dtm_lsa = Trunc_SVD(tf_vecs, iterations=5)\n",
    "plot_SVD(tf_dtm_lsa, title='Term Frequency - \\nInverse Document Frequency', level=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVD Value| CV | TFIDF')\n",
    "print('Top 2:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:2])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:2])),3))\n",
    "print('Top 3:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:3])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:3])),3))\n",
    "print('Top 4:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:4])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:4])),3))\n",
    "print('Top 5:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:5])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:5])),3))\n",
    "print('Top 6:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:6])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:6])),3))\n",
    "print('Top 7:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:7])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:7])),3))\n",
    "print('Top 8:  ',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:8])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:8])),3))\n",
    "print('Top 16:\\t',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:16])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:16])),3))\n",
    "print('Top 32:\\t',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:32])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:32])),3))\n",
    "print('Top 64:\\t',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:64])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:64])),3))\n",
    "print('Top 128:',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:128])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:128])),3))\n",
    "print('Top 256:',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:256])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:256])),3))\n",
    "print('Top 500:',round(sum(list(cv_dtm_lsa.explained_variance_ratio_[:500])),3),round(sum(list(tf_dtm_lsa.explained_variance_ratio_[:500])),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close look at the elbow plots\n",
    "def elbow(dtm_lsa):\n",
    "    evr = dtm_lsa.explained_variance_ratio_[:20]\n",
    "    print(\"Explained Variance Ratio (EVR):\\n\", evr)\n",
    "    print(\"Difference in EVR (start 3):\\n\", np.diff(evr[2:]))\n",
    "    plt.figure()\n",
    "    plt.plot(-np.diff(evr[2:]))\n",
    "    plt.xticks(range(-1,22), range(2,25))\n",
    "    plt.suptitle('Difference in Explained Variance Ratio', fontsize=15);\n",
    "    plt.title('Start from 3, moves up to 20');\n",
    "\n",
    "# Count Vectorizer\n",
    "elbow(cv_dtm_lsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow(tf_dtm_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_corpus = matutils.Sparse2Corpus(cv_vecs)\n",
    "pickle.dump(cv_corpus, open('../data/pretrained/cv_corpus_v4.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v,k) for k, v in cv.vocabulary_.items())\n",
    "id2word = corpora.Dictionary.from_corpus(cv_corpus, id2word = id2word)\n",
    "pickle.dump(id2word, open('../data/pretrained/id2word_v4.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = matutils.Sparse2Corpus(tf_vecs)\n",
    "pickle.dump(tfidf_corpus, open('../data/pretrained/tf_corpus.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_id2word = dict((v,k) for k, v in tfidf.vocabulary_.items())\n",
    "tf_id2word = corpora.Dictionary.from_corpus(tfidf_corpus, id2word = tf_id2word)\n",
    "pickle.dump(tf_id2word, open('../data/pretrained/tf_id2word.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus = cv_corpus, id2word = id2word, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_corpus = lsi[cv_corpus]\n",
    "doc_vecs = [doc for doc in lsi_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(lsi.print_topic(i, topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(doc_vecs)\n",
    "docu = 0\n",
    "sims = sorted(enumerate(index[doc_vecs[docu]]),key=lambda item:-item[1])\n",
    "np.r_[sims[:10],sims[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim_doc_id, sim_score in sims[:11]: \n",
    "    print(\"\\nScore:\", sim_score)\n",
    "    print(\"Document Text:\\n\", df.text[sim_doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaMulticore(corpus=tq.tqdm(cv_corpus), num_topics=15, id2word=id2word, passes=85, \n",
    "                              workers=13, random_state=42, eval_every=None, chunksize=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
